\chapter{The computational cytomorphology of myelodysplastic syndromes}

\section{Contribution and disclaimer}

In this chapter I present the outline and validation of a method I developed to detect and characterize cells in \ac{wbs}. Additionally, and using large collections of cells detected using this method in normal individuals and individuals with megaloblastic anaemia, iron deficiency anaemia, and different subtypes of \ac{mds} (\ac{sf3b1}-positive and \ac{sf3b1}-negative), I demonstrate how it is possible to discriminate all conditions with high predictive performance. Finally, I will be presenting \ac{mile-vice}, a weakly-supervised \ac{ml} framework to cluster cells into virtual cell types which can be used to derive cellular archetypes that characterize specific conditions and enable the discovery of novel cytomorphological markers.

\section{Introduction}

\ac{mds} are a heterogeneous group of myeloid neoplasms. The characterization of \ac{mds} encompasses several distinct clinical observations: consistently decreased numbers of mature blood cells (cytopenia) associated with a decreased ability of the bone marrow to produce blood cells (bone marrow failure), identification of blood cells from one or more specific lineages with abnormal cytomorphology (dysplasia) and an increase in the probability of \ac{aml} onset due to genetic instability \cite{Valent2017-uh,Hofmann2005-vv,Aster2020-cu}. In other words, \ac{mds} are simultaneously an oncological malignancy and a precursor to more serious forms of cancer.

The diagnosis of \ac{mds} is done through a number of complementary assessments and analyses. Blood counts, generally obtained through the use of an automated blood counter and can reveal anaemias, leukopenias and/or thrombocytopenias (cytopenias affecting \ac{rbc}, \ac{wbc} and platelets, respectively), all of which are common in \ac{mds} \cite{Najean1989-qm,Campo2017-wi}. Megaloblastic anaemias, in particular, can be indicative of or easily confounded with \ac{mds} \cite{Kaferle2009-pl,Vasekova2016-vo,Corey2007-cs}. The analysis of \ac{wbs} or bone marrow slides with different stains allows the detection of specific dysplastic features in blood cells. The former allows the characterization of abnormalities such as granulocytes with reduced nuclear segmentation, abnormal granularity in neutrophils or abnormal \ac{rbc} (such as macrocytes, elliptocytes and dacrocytes) \cite{Campo2017-wi,Langenhuijsen1984-qx,Kuriyama1986-ts,Davey1988-zn}. The analysis of bone marrow slides enables the identification of abnormal cell maturation or an excessive count of blasts \cite{Aster2020-cu}. Cytochemistry and immunohistochemistry can also be used with bone marrow slides and allows the identification of, among others, ring sideroblasts, which define a subtype of \ac{mds} \cite{Campo2017-wi,Mufti2008-ye}. Additionally, flow cytometry, cytogenetic characterization and molecular sequencing are necessary for the classification of \ac{mds} \cite{Aster2020-cu,Porwit2014-zi,Greenberg2012-en}, with multiparametric flow cytometry being often times considered essential for the diagnosis of \ac{mds} \cite{Cremers2016-fs}.

While analysis of WBS in the diagnosis of \ac{mds} typically has high concordance, the classification and characterization of individual cells is often times lacking in this regard \cite{De_Swart2017-wc,Howe2004-mn}. Additionally, the literature offers conflicting evidence on whether trained experts can agree on the distinctions between specific cell types, particularly monocyte subtypes \cite{Goasguen2009-dn,Foucar2020-uz}, while a study looking specifically at cell type classification concordance among 28 morphologists identified that experts agreed on only 60\% of all classified cells. This considerable level of inter-individual variability can create challenges in identifying novel blood cell subtypes that are relevant for the diagnosis of \ac{mds}, as well as in the routine characterization of different subtypes of \ac{mds} by untrained or poorly trained experts, creating the ideal circumstance for computational and reproducible approaches to be tested.

A recent work showed how previously established and expert-derived cytomorphological descriptions of \ac{wbs} could be used to classify \ac{mds} into specific and genomically-relevant subtypes with limited concordance in a validation cohort \cite{Nagata2020-lh}. While this approach can be potentially useful, it represents a laborious and time-consuming task for the trained expert. Additionally, the subpar inter-individual concordance in classifying cell types can hinder the possibility for finer-grained identification of cytomorphological determinants of \ac{mds} subtypes, novel or otherwise. Other studies have shown how the analysis of bone marrow slides using \ac{dl} can be leveraged to not only classify diseases, but also to detect the presence of specific mutations in both \ac{aml} and \ac{mds} \cite{Bruck2021-fx,Eckardt2021-fb}. These approaches are mostly possible due to the elevated density of nucleated cells in bone marrow --- this guarantees that even relatively small images (2560*1920 pixels) or with high resolution (50x), or splitting whole bone marrow slides into small tiles (224x224 and 299x299) will be representative of the whole bone marrow as in \cite{Eckardt2021-fb} and \cite{Bruck2021-fx}, respectively. The low density of nucleated cells in \ac{wbs} makes these approaches unfeasible.

\subsubsection{Deep-learning --- model training, overfitting and inference under low-data conditions}

In this chapter, a considerable portion of the methodology rests on \ac{ml} and \ac{dl}. As such, before presenting this, I discuss a few recurring features which come into play when training these models, especially in conditions where data acquisition and annotation is complicated. Particularly, it is worth highlighting that these methods, while extremely powerful, require vast amounts of data during training. This is a strategy that generically ensures the success of data-driven approaches --- by using increasingly larger amounts of data from diverse sources and sites, it is more likely that the model will generalize to newer data. However, in the life and biomedical sciences, this represents a circumstance which can be prohibitive, these fields are typically characterized either by the cumbersome and laborious acquisition and/or labelling of digital data and by the heavily biased acquisition of data. While the former is more easily solvable through training and given sufficient time, the latter is more complicated --- anecdotally, it was in 2016 that Yoshua Bengio advised against the training of more radiologists as they would soon (in 5 years) be outcompeted by artificial intelligence \cite{Creative_Destruction_Lab2016-zf} and in 2017 that Andrew Ng first hailed the end of the radiology profession \cite{Ng2017-tz}, but these rumours were greatly exaggerated. Indeed, it is quite common to see \ac{dl} models dramatically lose performance when confronted with independent test sets (data which the model has never seen) when the task is to classify X-ray images (it should nonetheless be noted that radiologists may also perform poorly) \cite{Rajpurkar2021-bj}. The reason for this is fairly illustrative --- most cohorts will be derived from single centres, with the assembly of large, multi-centre cohorts of digitalized image data representing a fairly modern and recent endeavour --- examples of this include The Cancer Genome Atlas. The reason why multi-centre approaches became desirable is due to the high variability in technical aspects concerning staining, image quality, image compression and scanning conditions and equipment \cite{Van_der_Laak2021-id}.

\paragraph{Underfitting and overfitting.} A classification model is not only concerned with learning a function of the data that returns the correct class; it should also be concerned in avoiding a relatively large drop in predictive performance on data that was not used to train the model due to overfitting --- the model learns a representation of the data that is perfect for the training data but relatively useless for new data --- and underfitting --- the model is far too simple and fails to capture the complex relationships that would allow it to correctly classify new examples. A more mathematically correct terminology for this would be to talk about the bias and variance of a model --- simply put, the bias measures the distance between predictions and target classes, whereas the variance measures differences in the trained model when different sets of data are used (i.e. if subsets of the training data are removed, how much does this impact the classification). Both of these are the two sides of the bias-variance trade-off --- it is not possible to maximize both at once, with low bias models running the risk of overfitting (high variance) and low variance models running the risk of underfitting (high bias) \cite{James2013-py}. While underfitting can be solved with a more complex parametrization of a model (in the case of \ac{dl} this often translates into increasing the number of parameters that the model has to learn), overfitting is slightly more complicated.

\paragraph{Regularization.} There are several ways to prevent overfitting when collecting or annotating more data is impossible. Some of these ways are more generic and can be applied to a diverse array of \ac{ml} models, whereas others are only applicable to models that work on image data. Regularization techniques belong to the former and their objective is to constrain or shrink coefficients, forcing models to learn much simpler representations of the data. More formally, given an optimization problem where the objective is to find a set of parameters $\theta$ that minimizes $l(X,y)$, an objective or loss function of the data $X$ and its corresponding annotations $y$ that measures the distance between predictions and target annotations (in other words, $\mathrm{arg min}_{\theta}l_{\theta}(X,y)$), the regularization will be concentrated on minimizing the magnitude of the coefficients. Typically, this regularization can be based on the $L_1$-norm ($L_1(\theta)$ regularization; $\sum_i^k{|\theta_i|}$) or on the $L_2$-norm ($L_2(\theta)$ regularization; $\sum_i^k{(\theta_i)^2}$) and, in the case of \ac{dl} and other gradient-descent based methods, can be added to the objective function when minimizing it, making it $\mathrm{arg min}_{\theta}(l_{\theta}(X,y)+\lambda L_2(\theta))$, where $\lambda$ is how much the regularization will contribute to the loss \cite{James2013-py}. The reason why I represented the $L_2$, rather than the $L_1$ regularization is because the standard application of gradient-descent algorithms require the loss function to be completely differentiable and $L_1$ is not differentiable whenever any parameter approximates $0$. Dropout is another method that can help in the training of \ac{dl} networks --- for each training iteration, a proportion $p$ of all neurons is randomly selected and set to $0$, forcing other neurons to learn representations which are both sparse and uncorrelated from those of other neurons \cite{Srivastava2014a}. Other methods which are fairly model-agnostic are those concerned with dimensionality reduction, where a low-dimensional and typically uncorrelated representation of the data is inferred using techniques such as principal component analysis. However useful these may be, I did not use them in this dissertation and, as such, will not further discuss them.

\paragraph{Reducing overfitting with image data and in deep-learning.} The wealth of methods to reduce overfitting when working with image data has become considerable, with these methods becoming increasingly complex. A comprehensive description and survey of these methods --- image augmentation methods --- would be interesting, but others already provide one \cite{Shorten2019-hr}; as such, I will focus only on giving a brief overview of these techniques. Simpler manipulations of the image (such as hue and brightness manipulation, Gaussian noise, random depletion of pixels or image regions, and random image cropping, resizing and flipping) are still fairly common, but others involving \ac{dl}-based augmentation techniques have become increasingly popular, including within the field of biomedical research \cite{Shorten2019-hr}. The simpler manipulations focus on the manipulation of the image as a digital object and most operations are linear or boolean transformations of all or individual pixels without assuming any form of dependence or structure. These can be useful when, for example, the objective is to train a model that classifies histopathology images and is capable of generalizing to different amounts of stains (assuming these are linearly separable), or to different illumination conditions during digitalization. The other methods, based on \ac{dl}, typically learn how to generate new examples or transform existing examples of training data by learning about the contextual information on the image.

Early stopping --- halting the training after some epochs of confirmed convergence --- has also been considered as good method to reduce over-fitting, but results seem mixed in terms of improving the predictive performance \cite{Prechelt2012-xf}. Fine-tuning and transfer learning, two training schemes which rely on \ac{dl} networks pre-trained on large collections of data, have also become increasingly popular. While similar, both rely on slightly different principles --- the focus of fine-tuning is to, given a pre-trained network, slightly adjust all the weights with a relatively low learning-rate in such a way that the predictive performance of the network is maximized for a new task; in other words, rather than randomly initializing the weights in a \ac{ann}, fine-tuning initializes the weights to an already good solution. Transfer learning focuses on training only a set of layers in a pre-trained model under the assumption that the feature representation that non-trained layers derive from the data is sufficient for the problem at hand. While transfer learning has the advantage of being less expensive from a computational perspective, it is also more dependent on how useful the pre-trained feature representation is \cite{transfer-learning-fine-tuning}. 

To further alleviate the burden of the biases present in the training data, simple aspects such as class balance should also be considered --- indeed, the existence of considerably more examples of one class compared to the rest may lead to a poorly calibrated model if these proportions are not reflected in the real world \cite{Van_Calster2019-zp}. For instance, if a model was very accurate at discriminating between foxes and dogs but was trained on a city sample of many dogs and very few foxes, it may perform poorly during a forest where foxes are much more common than dogs. To prevent this, weighing each instance according to its class probability (such that lesser represented classes contribute as much to the learning as more represented classes) can be helpful, with other methods such as data sampling schemes focusing on under-represented or worse performing classes also being popular \cite{Johnson2019-cf}. Finally, it is important to consider that, while \ac{dl} networks try to learn representations that are rotation- and scale-invariant, this invariance is not perfect and its relevance is context-dependent. For instance, if a model is trained for face identification (determining whether a face belongs to an individual), it is often the case that rotation-invariance is not necessary because the input is seldom rotated or flipped, but scale-invariance is convenient because a face can be captured at different distances. However, if the model is trained to detect or segment cells spread over a microscope slide obtained at constant resolution, then rotation-invariance becomes a key concern, whereas variations in scale are virtually non-existent. \Ac{tta} uses this information to improve the predictive performance of algorithms during inference --- with \ac{tta}, the data is artificially augmented during inference --- scaled, rotated, flipped, shifted or warped --- in order to create a consensus prediction. While relatively simple, \ac{tta} has been shown to improve predictive performance in classification, segmentation and object detection tasks \cite{Moshkov2020-rc,Shorten2019-hr}.

\section{Methods for the detection of blood cells in whole blood slides}

\subsection{Data collection and definition of cohorts}

This work required access to a sufficiently large collection of \ac{wbs}. To this effect, during this work I had access to three cohorts:

\begin{itemize}
    \item \textbf{AC1} --- 55 \ac{wbs} from normal individuals used to develop the \ac{wbc} and \ac{rbc} detection pipelines detailed below (digitalized by Jonathan L. Cooper at the Munich Leukaemia Laboratory over two weeks with a Hamamatsu Nanozoomer 2.0). This cohort was used to develop the computational pipeline for blood cell detection;
    \item \textbf{MLLC} --- 354 \ac{wbs} from individuals with \ac{mds} with mutations in either \ac{sf3b1}, \ac{srsf2}, \ac{u2af1} or \ac{runx1}, iron deficiency anaemia, megaloblastic anaemia and controls (digitalized by myself at the Munich Leukaemia Laboratory over two weeks with a Hamamatsu Nanozoomer 2.0). This cohort was used to develop the computational algorithms for predicting different clinical conditions;
    \item textbf{AC2} --- 68 \ac{wbs} from individuals with \ac{mds} with mutations in either \ac{sf3b1}, \ac{srsf2}, \ac{u2af1} or \ac{runx1}, iron deficiency anaemia, megaloblastic anaemia and controls (digitalized by Dr. Emma Gudgin at the Munich Leukaemia Laboratory over two weeks with a Hamamatsu Nanozoomer 2.0). This cohort was used to validate the computational algorithms for predicting different clinical conditions.
\end{itemize}

For MLLC and AC2 I also had access to blood counts (\ac{wbcc}, haemoglobin concentration and platelet counts) and the age and sex of each individual. Each slide was inspected individually and, if there was a problem covering the entire slide (i.e. cellular density was too high across the whole slide or the digitalization resulted in a consistently blurry slide) this was removed from further analysis.

\subsection{Computational detection and characterization of blood cells}

\subsubsection{Quality control of slide tiles}

Considering that the digitalization \ac{wbs} is automated and that the size of each \ac{wbs} is on the order of gigapixels, I process individual tiles --- small $h*w$ ($512*512$ in my case) regions of the \ac{wbs} --- individually to determine whether they should be further analysed. This step intends to exclude tiles which are either blurred, have very high cellular density or very low cellular density. While there are relatively simple ways to quantify blurriness, accurately identifying scenarios of very high or very low cellular density is not as trivial as this generally requires contextual information (whether there are cells or other specific objects in the image). Additionally, recent work has shown that \ac{dl} can accurately quantify the blurriness of an image in microscopy settings, outperforming other metrics devised to quantify blurriness \cite{Yang2018-ve}. As such and inspired by this work, I trained a classifier based on a densely connected layer network with 121 layers (DenseNet121) \cite{huang2017densely}, an \ac{ann} known for its relatively low parameter count while maintaining good performance and relatively quick prediction. To this effect I labelled 10,000 tiles from MLLC and classified them as either "poor quality" (blurry tiles, poorly illuminated tiles, tiles high or very low cellular density; \missing{how many}) or "good quality" (clear and sharp tiles, tiles with appropriate cellular density (i.e. at least a few cells which are not in close contact with other cells; \missing{how many}). This dataset was split into two separate training and testing sets based on the slide of origin, with 75 slides and a total of 8,050 tiles in the training set, and 18 slides and a total of 1,950 for the testing set. I trained this model for 25 epochs with a batch size of 32 and the Adam optimizer with an initial learning rate of 0.00005 that decayed by 90\% every time the training loss stopped decreasing. During training, each image had a 60\% probability of having its brightness, saturation, hue and contrast randomly altered (by 15\%, 10\%, 10\% and 10\%, respectively) or of having random JPEG compression artefacts introduced. This makes the training more robust to alteration of \ac{wbs} preparation and variability in image digitalization.

\subsubsection{Detection of red blood cells}

The detection algorithm for \ac{rbc} is relatively simple since the detection of all \ac{rbc} was not of interest to this project; rather, it was more important to detect separate and isolated \ac{rbc} in the \ac{wbs}. As such, I developed a two-step approach for this --- the first step involves the segmentation and detection of all objects which are likely to be isolated red blood cells using traditional \ac{cv} and morphological operators. The second step characterizes and classifies each object as "\ac{rbc}" or "not \ac{rbc}", to filter out all false positives.

\noindent \textbf{Segmentation and detection algorithm}

As mentioned, the protocol for \ac{rbc} segmentation is relatively simple and is best summarized in Algorithm \ref{alg:rbc-detection}. For clarity, $CannyEdgeDetector$ represents the Canny edge detection algorithm \cite{Canny1986-pi}, $getContours$ is a standard routine to detect object contours from a binary image, $drawContours$ is a routine to draw contours on the image and fill them, $getArea$ is a function that calculates the area of a contour or ellipse, $averageColour$ is a function that selects the pixels in an image belonging to a contour and calculates their average value, $fitEllipse$ is a function that fits an ellipse to a contour, $getMajorAndMinorAxis$ is a function that calculates the lengths of the major and minor axes of an ellipse and $isolateObject$ is a function that returns the region of the image containing the \ac{rbc} and its respective mask. 

\begin{algorithm}[!ht]
    \caption{Red blood cell detection algorithm.}\label{alg:rbc-detection}
    \KwData{$image$}

    \KwResult{Set of masked RBC images and their contours $S$}

    $S \gets \{\}$;

    $edgeImage \gets CannyEdgeDetector(image)$;

    $contours \gets getContours(image)$;

    $contourImage \gets drawContours(contours)$;

    $contourImage \gets contourImage - edgeImage$;

    $contours \gets getContours(contourImage)$;

    \For{$contour$ in $contours$} {
        $area \gets getArea(contour)$;

        \If{$area > 300px$ and $area < 2000px$};
        {
            $averageColour \gets getAverageColour(image,contour)$;

            \If{$averageColour > 170$ and $averageColour < 220$}
            {
                $ellipse \gets fitEllipse(contour)$;

                $majorAxis,minorAxis \gets getMajorAndMinorAxis(ellipse)$;

                $areaEllipse \gets getArea(ellipse)$;

                \If{$areaEllipse < 1500px$ and $areaEllipse > 300px$ and $MajorAxis/MinorAxis < 1.5$}
                {
                    $isolatedCellImage,isolatedCellMask \gets isolateObject(image,contour)$;

                    append $\{isolatedCellImage,isolatedCellMask,contour\}$ to $S$;
                }
            }
        }
    }
\end{algorithm}    

\noindent \textbf{Machine-learning-assisted filtering}

While simple, the protocol described above for \ac{rbc} detection can return objects which may not be \ac{rbc}. As such, I filter them out by characterizing them in terms of shape, colour and texture (details below) and use these features to train a classifier that predicts each object as being "\ac{rbc}" or "not \ac{rbc}". For this, I used \ac{xgboost}. \ac{xgboost} is a boosting algorithm which uses as its base learner decision trees, featuring tree penalization and pruning, high parallelization and implicit feature selection \cite{Chen2016-xk}. To train this algorithm, I picked a random set of 158 $2096*2096$ tiles from AC1 and split them into training (109) and testing tiles (48) and, for each tile, I run the \ac{rbc} detection pipeline described above and characterize each \ac{rbc}. I then labelled objects as "\ac{rbc}" (positive class) or "not \ac{rbc}" (negative class), with a total of 2262 objects labelled for training (positive class: 1870; negative class: 392) and 1107 (positive class: 962; negative class: 145). I train an \ac{xgboost} model with a maximum number of 50 weak estimators and DART boosting (used to ensure that earlier trees do not become considerably more important than later trees through the use of dropout) \cite{Rashmi2015-qe} on the training set and test them on the validation set. Additionally, given the relatively high class imbalance (approximately only 1 out of every 7 objects belongs to the negative class), the objective function of each positive sample was scaled by a factor of 0.1.

\subsubsection{Detection of white blood cells}

Considering that \ac{wbc} are more variable than \ac{rbc}, I trained a U-Net model \cite{Ronneberger2015-do} to segment \ac{wbc}. To this effect, I annotated all of \ac{wbc} in the 158 $2096*2096$ tiles mentioned above, yielding a total of 2853 annotated \ac{wbc}, 2747 of which were completely visible. To assess the performance of U-Net across all cohorts, I also labelled 2 images from MLLC (containing 60 \ac{wbc}) and 30 tiles from AC2 (containing 69 \ac{wbc}). Different architectures were tested to assess whether the depth (the number of retrieved features) of the network (i.e. the richness of the characterization) had a preponderant impact on the output; to this effect, different instances of a U-Net model were trained where the depths of each layer were multiplied by 1.0, 0.5 and 0.25 and rounded to the nearest integer. I trained each U-Net model on randomly rotated $512*512$ tiles extracted from the original tiles and an $L_2$-regularization of 0.005 with a weighted cross-entropy loss and using the Adam optimizer \cite{Kingma2014-zd} over 200 epochs. The learning rate for training was set to 0.0005 for depth multipliers of 0.25 and 0.5 and 0.0001 for the original depth (depth multiplier = 1). The weighted cross-entropy was weighted based on each pixel's relative distance to the nearest cell border --- particularly, the weight of pixels belonging to a cell was 1, whereas that of other pixels was calculated based on their distance to the nearest cell border $d$ as $w = 0.5 * (1-0.5)e^{-\frac{2d^2}{2*10^2}}$; in other words, pixels belonging to the cell and near the cell were more heavily considered during training as a mechanism to avoid the over-segmentation of cells (i.e. two cells being segmented as a single object) \cite{Ronneberger2015-do}. I also tested whether using \ac{tta} could improve results \cite{Moshkov2020-rc}, and assess the performance of all models at 75 to verify whether early stopping can lead to improved results in my case \cite{Prechelt2012-xf}. 

Training in any case was performed with real-time data-augmentation --- particularly, random alterations to the brightness, saturation, hue and contrast were introduced to each image (10\%, 10\%, 5\% and 10\%, respectively), random Gaussian noise with a standard deviation of 0.005 was added to the image, each image had the probability of being slightly blurred with a Gaussian filter (probability of 0.1\%; standard deviation of 0.005) and random JPEG compression artefacts were added. Additionally, each image is warped using a random elastic transform as suggested in the original U-Net paper \cite{Ronneberger2015-do} --- this creates small, local warping distortions in the image, increasing the amount of \ac{wbc} shape variability during training. 

All models were evaluated based on their mean \ac{iou} on independent test sets for all three cohorts. The \ac{iou} considers the intersection between the ground truth and the pixels predicted as belonging to the object of interest (\ac{wbc} in my case) and the union of these and calculates its ratio --- as such, a value of 1 implies a perfect overlap between ground truth and prediction, with the \ac{iou} decreasing as segmentation quality becomes worse.

In this work I also segment the nucleus of WBC. To this effect, I use the knowledge that the nucleus is darker than the rest of the WBC and, to segment it, I cluster the pixels on each segmented WBC using k-means clustering and two separate clusters, taking inspiration from a wealth of WBC segmentation techniques that are discussed elsewhere \cite{Andrade2019-qv}. Through this I can easily assign each pixel to the darkest (nucleus) or brightest (cytoplasm) part of the image.

During inference, I consider only objects with an area greater than 1000px and smaller than 8000px as being \ac{wbc}. These values were calculated from the size distribution of the \ac{wbc} I labelled for segmentation. Additionally, I use \ac{tta} to improve the robustness of the detected objects and to avoid segmenting objects on the edge of an image (which may be incomplete) I use a sliding window of size 640px with a 128px stride and remove objects detected as being on the edge of the sliding window. 

\subsubsection{Characterization of blood cells}

After detecting and adequately segmenting each cell as detailed in the previous section of the methods, I calculate for each cell a set of morphological descriptors. The descriptors used here were implemented in a custom Python script but the large majority of them are also present in bioimage analysis software programs \cite{Carpenter2006-hy,Sommer2011-ds} or described in publications reviewing morphometry in image analysis \cite{Mingqiang2008-wv}. Using the features described in \tableref{table:features}, I describe every RBC and WBC. Additionally, for each WBC I characterize its nucleus using a reduced set of features, also detailed in \tableref{table:features}. Finally, each \ac{wbc} is described by 53 features (42 for the cellular characterization and 11 for the nuclear characterization) and each \ac{rbc} by 42 features. To account for the different resolutions (0.2517 micrometers/pixel for Hamamatsu NanoZoomer 2.0; 0.2268 micrometers/pixel for \missing{scanner name}) I rescale each cell image in AC2 by a factor of 1.1098 prior to their characterization.

\begin{table}[!ht]
    \centering
    \caption{Features used for morphological characterisation.}
    \pgfplotstabletypeset[
    font=\footnotesize,
    string type,
    columns/f/.style={
        column name=Feature (count),
        column type={C{.2\textwidth}}},
    columns/e/.style={
        column name=Description,
        column type={C{.65\textwidth}}},
    columns/n/.style={
        column name=Nuclear (count),
        column type={C{.05\textwidth}}},
    every head row/.style={before row={\toprule},after row=\midrule},
    every last row/.style={after row={\toprule}},
    every odd row/.style={before row={\rowcolor[gray]{0.9}}}
    ]\featuresMorphology
    \label{table:features}
\end{table}

\subsubsection{Morphometric distributions}

I define morphometric distributions as parametric characterizations of the distribution of each feature across different whole blood slides. To do so, I calculate for all individual features retrieved from RBC and WBC collections their mean and variance. This way I obtain a relatively simple characterization of the distribution of each feature which can be used in predictive modelling tasks as features. To ensure that estimates for mean and variance are adequate, I exclude all cases where fewer than 50 \ac{wbc} or \ac{rbc} were detected. 

\subsubsection{Pipeline description and method implementation}

Summarily, the pipeline for blood cell detection, implemented in Python and managed by Snakemake, is described below, with one point for each script used:

\begin{enumerate}
    \item Each $512*512$ tile is classified as being of poor or good quality and this classification is stored as a comma-separated values files;
    \item Each good quality tile is analysed using the \ac{rbc} and \ac{wbc} detection protocols and tile regions containing cells are stored in separate HDF5 files (one for \ac{rbc}, another for \ac{wbc}), thus enabling quick access;
    \item Each cell on each of the HDF5 files described in the point above is characterized using separate protocols:
    \begin{itemize}
        \item The \ac{wbc} characterization protocol first segments the nucleus and then characterizes the whole cell and the nucleus separately
        \item The \ac{rbc} characterization protocol only characterizes the whole cell.
    \end{itemize}
    \item The mean and variance of each feature is calculated.
\end{enumerate}

All \ac{dl} networks are implemented using Tensorflow 2.0 \cite{tensorflow2015-whitepaper} and are available through Github (Code Availability statement is presented at the end of this chapter). \Ac{cv} operations were implemented using both OpenCV2 \cite{opencv_library} and scikit-image \cite{van2014scikit}, and slide I/O operations were handled by OpenSlide \cite{Goode2013-zs}.

\section{Methods for the prediction of clinical conditions using collections of cells}

In this work, I defined four distinct tasks to assess how well computational cytomorphology could be used to predict different clinical conditions based on the data available from the MLLC:

\begin{enumerate}
    \item Disease detection --- identifying the presence of either anaemia or \ac{mds} (easy for trained experts);
    \item Disease classification --- distinguishing between anaemia and \ac{mds} (hard for trained experts --- anaemia can be a symptom of \ac{mds});
    \item \ac{sf3b1}mut \ac{mds} detection --- distinguishing between \ac{sf3b1}-mutant \ac{mds} and other subtypes of \ac{mds} (hard for trained experts --- no known morphological manifestation of \ac{sf3b1} mutations exist);
    \item Anaemia classification --- distinguishing between iron deficiency anaemia and megaloblastic anaemia (easy for trained experts).
\end{enumerate}

In addition to the cytomorphological features already described, I also had access to a few \ac{cbc} parameters, particularly \ac{wbcc} (cells/uL), haemoglobin concentration (g/dL) and platelet counts (platelets/uL). In this work, I also assessed their impact on the predictive performance of all the classification methods. For clarity, I do not report training metrics and focus exclusively on cross-validated metrics or external validation metrics.

\subsection{Classification using supervised classification methods}

For the supervised classification methods, my input was, for each individual, a vector containing the mean and variance of each feature. This constitutes, for the cytomorphological features, a total of (53 + 42) * 2 = 190 features considering both \ac{wbc} and \ac{rbc} which, when combined with the \ac{cbc} data, yielded a total of 193 features. To get different baselines, I defined three datasets --- one composed of \ac{cbc} features (3 features), one composed of cytomorphological features (190) and one composed of both feature types (193). 

To assess the predictive performance of these features for each one of the tasks described above, I used 5-fold cross-validation (splitting the data into 5 non-overlapping sets which will be used as validation sets for 5 rounds of training performed on the rest of the data). I started by splitting the data into 5 non-overlapping test sets using a stratified cross-validation scheme, thus ensuring that each training and validation sets had a sufficient number of each class. Following this, I preprocessed the data on each training set in order to standardize it and remove features which are highly correlated ($|R|>0.9$) with other features. Additionally, and considering that \ac{cbc} was available for all individuals, I imputed the missing values for \ac{cbc} data using the median for each parameter. I train two different models for each round of cross-validation --- a binomial regression model with an elastic network regularization (a weighted combination of the $L1$ and $L2$ regularizations) as implemented in the \texttt{glmnet} package for R \cite{Friedman2010-gl} and a random forest model \cite{Breiman2001-yz}. Random forests are ensemble models which combine the weighted predictions from several decision trees trained on a random subset of data and features. The random nature of the training makes them extremely robust to overfitting since it prevents a subset of data or of features from having a disproportionate amount of effect on the trained model. To deal class imbalance I weighted the objective of each sample of $class_i$ by $1-\frac{number of elements in class_i}{total number of elements}$. 

After training each model, I calculate the \ac{auroc} by using the concatenated results from all cross-validation rounds to obtain the cross-validated \ac{auroc} \cite{Fawcett2006-mo}. However, for inference and assessment of feature importance I select the best performing model out of each set of cross-validated models. I assessed the feature importance for the binomial regression by calculating the standardized value for each coefficient and for the random forests by using the mean decrease in accuracy as recommended by Breiman in the original paper describing random forests \cite{Breiman2001-yz}. To assess the importance of a group of features --- the total explained variance for each group --- I use the trained binomial regression model and calculate the effect of each group of variables by summing the effects of the standardized variables composing it for each individual. Finally, I calculated the total explained variance by each group of features by calculating the sum of the rows in the covariance matrix between all 5 groups of features --- \ac{wbc} mean, \ac{wbc} variance, \ac{rbc} mean, \ac{rbc} variance and \ac{cbc} parameters.

\subsection{MILe-ViCe --- Multiple Instance Learning for Virtual Cell quantification}

\subsubsection{Motivation}

Each slide, from the blood cell detection pipeline specified above, is described as a set of $m$ RBC and $n$ WBC. Typically, when diagnosing \ac{mds}, an expert focuses on identifying abnormal cell types and on quantifying their relative prevalence \cite{Valent2017-uh}. In more abstract terms, given a set of objects, the expert has to characterize and classify each object and use this information to infer the prevalence of specific abnormalities in this set of objects, thus obtaining a classification for the set of objects that is dependent on the prevalence of different classes. In other words, this is essentially a problem of \ac{mil} --- a learning task that focuses on the characterization of objects (or instances) in a group to predict a classification for that group. Some \ac{mil} methods combine this group classification task with the auxiliary task of learning a function that maps instances to a vocabulary and uses the presence or frequency of instances in each “concept” of the vocabulary to predict a classification for the group \cite{Amores2013-ym}. Having this abstraction is helpful to devise an approach that is not only able to classify \ac{wbs} into separate classes (i.e. “normal” vs. “disease”) but also to classify each individual instance into specific concepts (or, in my case, cells into specific virtual cells) that are helpful in the classification process. 

\subsubsection{Multiple instance learning}

While traditional algorithms for supervised \ac{ml}, are dedicated to learning a function that maps a single instance to a class (a one-to-one mapping such as a picture of a cat to the class "cat"), \ac{mil}, a type of supervised \ac{ml}, learns how to map a set of instances to a single class. Essentially, the objective of \ac{mil} algorithms is to take a set of data points --- a bag --- and assign this bag to a class, rather than each individual data point to a class. It is possible to split these methods based on how they handle each individual element (whether they are independent or whether they should be classified individually, for example) and based on how the classification for the bag is calculated (whether the classification should depend on the presence/absence of specific elements or on their relative proportions or whether the interest lies instead in detecting similarity within each bag) \cite{Amores2013-ym,Carbonneau2016-xc}. While problems such as protein sequence classification or sentiment analysis are good examples of \ac{mil} when the bags are inherently structured, I am considerably more interested in the case where no structure is identifiable --- indeed, I am interested in classifying a "bag of cells" stratified by automatically detected \ac{wbc} and \ac{rbc} in a \ac{wbs}. Not only that, but I am also interested in the determination of a vocabulary --- a correspondence between the features that characterize a cell and a virtual cell type, a computational construct that is morphologically coherent --- that is relevant for the classification task. In the ideal case scenario, this vocabulary-based method would map individual cells to specific classes (virtual cell types) and use the proportion of each virtual cell type to classify a bag as having specific haematological conditions, thus combining good predictive performance with the identification of relevant virtual cell types. The main advantage of this is that no it allows for the discovery of novel cytomorphological signatures which can be adapted to the clinic.

\subsubsection{Model specification}

Consider a \ac{wbs} $W$ characterized as a set of $n$ cells $W=\{c_1,c_2,...,c_n\}$, with each cell characterized by a vector of $f$ features. A function $C$ maps each cell $c_i$ to a concept (virtual cell) $v_i$ given a vocabulary $theta$ with $m$ concepts such that $v_i = C(c_i,\theta)$, with $\sum_{j=1}^m{v_i} = 1$ --- in other words $v_i$ describes the membership of $c_i$ to a set of $m$ concepts. As such, each \ac{wbs} is now characterized as a set of $n$ concept memberships $W'=\{v_1,v_2,...,v_n\}$. Having identified the concept to which each cell belongs to, it is now possible to calculate the proportion of concepts in $W$ by calculating $V = \frac{1}{n}\sum_{i=1}^n{v_i}$. This vector of proportions can now be used to calculate the probability $p$ of $W$ having been prepared from an individual with one of $k$ conditions such that $p = P(V)$.

I am interested in learning $C$ and $P$ at the same time, thus enabling this approach to both predict conditions from specific slides and learn which virtual cell types are relevant to do so. As such, I need to define $C$ and $P$ in a way that allows concurrent optimization. I also need to consider that there are \ac{wbs} with tens or hundreds of thousands of detected  cells. Because of this, I am interested in selecting a method that enables randomly selected subsets of cells to be used in each training iteration in such a way that converges towards an optimal solution.  Finally, I have to consider the fact that I am interested in characterizing \ac{wbs} and \ac{rbc}. To this end, I parametrize $C_{WBC}$, $C_{RBC}$ and $P$ as linear functions with softmax transforms, such that $C_{WBC}$ and $C_{RBC}$ takes vectors of features characterizing \ac{wbc} $wbc_i$ and \ac{rbc} $rbc_j$, respectively, with each producing a vector of length $m$ containing the probabilities of $wbc_i$ and $rbc_j$ belonging to a specific virtual cell from two distinct vocabularies characterizing \ac{wbc} and \ac{rbc} --- $v_{WBC(i)}$ and $v_{RBC(j)}$, respectively. $P$ takes the concatenated vector of virtual cell proportions for \ac{wbc} and \ac{rbc} $V=concatenate([VWBC,VRBC])$ and produces a prediction $p$. 

This formulation also enables the concatenation of additional information --- such as blood counts --- to $V$ and guarantees that the entire system can be optimized using stochastic gradient descent, also enabling the utilization of randomly selected subsets of \ac{wbc} and \ac{rbc} on each training step. The missing element in this system is defining the optimal number of virtual cell types; I consider this to be a hyperparameter in this model.

\subsubsection{Model optimization}

I test each task using different numbers of virtual cell types --- $[10, 25, 50]$ --- assuming that both \ac{wbc} and \ac{rbc} can be clustered into the same number of virtual cells. I also test whether blood counts (WBC counts (cells/uL), haemoglobin concentration (g/dL) and platelet counts (platelets/uL)) improve the classification performance of \ac{mile-vice}. I standardize (mean = 0 and standard deviation = 1) all additional continuous variables --- \ac{wbcc}, haemoglobin concentration, platelet counts or age. 

From solving these four tasks separately, I can expect the best possible outcome to be four separate models, each of which is primed to solve its task and to identify virtual cell types which are relevant for the detection of a specific condition. However, I was also interested in the virtual cell types which are exclusively useful for one task; to this effect, I train additional models under two specific settings --- one that is a multiclass model, similar to that describe in the previous section, and one that focuses on the concurrent optimization of all four tasks using multi-objective learning. Regarding the multi-objective learning setting, in each training step and for each of the four tasks I sampled a set of relevant \ac{wbs} to be considered, predicted the outcome class and calculated the cross-entropy. I then add all four loss values after multiplying them by a vector sampled from a Dirichlet distribution with concentration [1,1,1,1], a trick proposed in \cite{Ruchte2021-ch} to approximate Pareto optimality (i.e. no improvement can be done to one task without making a different task worse \cite{Censor1977-nd}) and backpropagate this value. For this scenario I also test the inclusion of blood counts and demographic information in the final vector $V$.

To select the best model in each setting I use the cross-validated \ac{auroc}. To get these cross-validated scores, each model is trained over 5 folds and the average value and standard error of each metric is calculated. In each fold, a model is optimized for a maximum 15,000 steps with a learning rate of 0.01 and a weight decay of 0.25. I use a weighted cross-entropy loss and the Adam optimizer \cite{Kingma2014-zd}. The weight for the positive class is calculated as $1 - \frac{no.\ elements\ in\ positive\ class}{total\ number\ of\ elements}$. Additionally, the learning rate is decreased every time the training loss stagnates for at least 100 steps by a factor of 0.5. If the learning rate reaches a value of 0.000001 (smaller than the initial learning rate by 4 orders of magnitude). On each training iteration I sample without replacement $500$ \ac{wbc} and $500$ \ac{rbc}. Whenever fewer than $500$ cells were detected, I sample with replacement. This may lead to a situation where bags of cells with a smaller amount of information (fewer cells) contribute equally to those with a large amount of cells. To minimize this, I weigh each instance with a weight $w_{avail}$ calculated from the number of available \ac{rbc} $m$ and \ac{wbc} $n$ such that $w_{avail} = 1 + \frac{minimum(500,m) + minimum(500,n)}{500*2}$.

\subsubsection{Virtual cell validation}

To validate the cohesiveness of virtual cells I asked \missing{how many} haematologists to annotate between 350 and 1,600 \ac{wbc} and \ac{rbc} automatically detected using the protocol described above from MLLC. I then calculated how these cells were distributed across the different virtual cell types. Ideally, different virtual cell types should be constituted predominantly by a specific type of \ac{wbc} or \ac{rbc}. For the annotation, I implemented an online platform --- \ac{tbca} --- where haematologists could register and annotate cells according to previously defined cell types discussed with my co-advisor George Vassiliou. More particularly, \ac{wbc} could be classified as neutrophils, basophils, eosinophils, band cells, lymphocytes, monocytes, blasts or artefacts (poor resolution, nucleated \ac{rbc}, reticulocytes, fragmented or dead \ac{wbc}, multiple \ac{wbc}, incomplete or no \ac{wbc} and platelet clumps). \Ac{rbc} could be classified as normal \ac{rbc}, nucelated \ac{rbc}, spherocytes, target cells, irregularly contracted, dacrocytes, keratocytes, echinocytes, eliptocytes, acanthocytes and artefacts (poor resolution, multiple \ac{rbc}, incomplete or no \ac{rbc} and platelet clumps, platelets or oversegmented \ac{rbc} with platelets) . Whenever a cell had more than one annotation, I used a majority rule to decide which annotation to keep. To address ties, I select a label at random.

\subsection{Statistical analysis}

All statistical analyses were performed in R 3.6.3 \cite{R-core-team} --- \texttt{caret} was used to calculate cross-validated metrics \cite{Kuhn-2021-caret}, \texttt{MASS} was used to calculate robust $R^2$ values \cite{Venables-2002-mass} and \texttt{pROC} was used to calculate \ac{auroc} \cite{Robin-2011-proc}. 

\section{Results}

\subsection{The MLLC recapitulates blood count trends observed in other studies}

To confirm that the MLLC is a representative cohort, I start by analysing a few simple aspects of this cohort. Particularly, I start by confirming that individuals with \ac{mds} are generally much older than the rest of the population with a particular bias towards male individuals, as previously reported \cite{Rollison2008-yg}. Indeed, the chance of having \ac{mds} in our cohort increases by 12\% every year, with male individuals being more than twice as likely to contract \ac{mds} (according to a binomial regression with binary \ac{mds} status as the dependent variable and age and sex as the covariates, $p=8.47*10^{-16}$ and $p=0.000173$, respectively; \figref{fig:mds-age} and \figref{fig:mds-sex}). While recapitulating these patterns confirms that our cohort is representative, it also prevents us from using both parameters as features in predictive modelling as they might bias the results.

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/mllc-age.pdf}
    \placecaption{Age distribution for the different conditions in MLLC.}
    \label{fig:mds-age}
\end{figure}

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/mllc-sex.pdf}
    \placecaption{Sex distribution for the different conditions in MLLC.}
    \label{fig:mds-sex}
\end{figure}

I additionally observe that individuals with either \ac{mds} or anaemia have a tendency to be leukopenic, with individuals having approximately 1,200 and 1,800 fewer \ac{wbc}/uL in these conditions, respectively (according to a linear regression where WBC count is the dependent variable and binary status as \ac{mds} and anaemia are the dependent variables, $p=0.0379$ and $p=0.0088$, respectively; \figref{fig:mds-wbcc}). However, this leukopenic tendency in anaemias is driven by individuals with megaloblastic anaemia --- indeed, whereas iron-deficient individuals are indistinguishable from normal individuals, individuals with megaloblastic anaemia have approximately 3,200 WBC/uL fewer than normal individuals ($p=6.224*10^{-14}$ for a two sample t-test) --- an effect that has been reported elsewhere and that can be associated with folic acid or vitamin B12 deficiencies \cite{Kaferle2009-pl,Castle1978-ky} and that further highlights how megaloblastic anaemias can be confounded with \ac{mds}. Haemoglobin concentrations are also much lower in \ac{mds} and, expectedly, anaemias (\figref{fig:mds-hb}) --- indeed, these individuals have 4.34 and 6.38 fewer haemoglobin g/dL, respectively (according to a linear regression where Hb concentration is the dependent variable and binary status as \ac{mds} and anaemia are the dependent variables, $p<2*10^{-16}$ and $p<2*10^{-16}$, respectively). No detectable difference between normal individuals and individuals with either \ac{mds} or anaemia is observable when it comes to platelet counts, but it should be noted that individuals with megaloblastic anaemia have approximately an expected 146,000 fewer platelets/uL than normal individuals ($p=3.312*10^{-12}$ for a two sample t-test; \figref{fig:mds-plt}), concordant with previous reports \cite{Castle1978-ky}. 

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/mllc-wbcc.pdf}
    \placecaption{White blood cell count distribution for the different conditions in MLLC.}
    \label{fig:mds-wbcc}
\end{figure}

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/mllc-hb.pdf}
    \placecaption{Haemoglobin concentration distribution for the different conditions in MLLC.}
    \label{fig:mds-hb}
\end{figure}

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/mllc-platelet.pdf}
    \placecaption{Platelet count distribution for the different conditions in MLLC.}
    \label{fig:mds-plt}
\end{figure}

Finally, I also observe that \ac{sf3b1}-mutant \ac{mds} presents clinically distinct features from other \ac{mds} subtypes --- particularly, WBC and platelet counts are indistinguishable from those of normal individuals while being much higher than those found in other \ac{mds} subtypes ($p=0.3372$ and $p=0.1506$ for two sample t-tests comparing \ac{wbcc} and platelet counts between individuals with \ac{sf3b1}-mutant \ac{mds} and normal individuals; $p=0.001734$ and $p<2.2*10^{-16}$ for two sample t-tests comparing \ac{wbcc} and platelet counts, respectively, between individuals with \ac{sf3b1}-mutant \ac{mds} and individuals with other \ac{mds} subtypes in our cohort). This is in agreement with what has been previously described in terms of higher platelet and absolute neutrophil counts in \ac{sf3b1}-mutant \ac{mds} \cite{Malcovati2015-tz,Malcovati2020-no} and further highlights the relevance of being able to discriminate between this and other \ac{mds} subtypes. 

\subsection{Automated cell detection captures thousands of cells per whole blood slide}

\subsubsection{Deep-learning-assisted quality control}

I trained a model based on DenseNet121 to classify whether individual tiles from slides were of good or poor quality (\figref{fig:slide-quality-examples}). Validation shows good predictive performance with an \ac{auroc} of 93.4\%, recall of 82.2\% and precision of 85.2\%. In other words, this model is better at identifying poor quality slides than good quality slides. This leads to a tolerable excess of false negatives when compared with false positives --- given the problem at hand (the morphologic characterization of \ac{wbc} and \ac{rbc}), it is preferable to avoid false positives as this also avoids the detection of downstream false positives (i.e. objects in the \ac{wbs} which are not blood cells), thus minimizing the detected artefactual morphological signatures.

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/slide-quality-examples.pdf}
    \placecaption{Examples of good and poor quality tiles used in training and testing with classifications.}
    \label{fig:slide-quality-examples}
\end{figure}

An overview of the slide regions classified as good quality further confirms the quality of the predictions \figref{fig:slide-quality-regions} --- mostly, the regions corresponding to the monolayer, a region of the slide where a good number of non-overlapping cells are observable and that is usually preferred for \ac{wbs} analysis by haematologists \cite{Adewoyin2014-vo}, is the one identified by this approach.

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/slide-quality-regions.pdf}
    \placecaption{Examples of regions of the slide labelled as being of good or poor quality (brighter regions on the quality map were predicted as being of good quality).}
    \label{fig:slide-quality-regions}
\end{figure}

\subsubsection{Red blood cell detection and machine-learning-assisted filtering}

Individual \ac{rbc} were detected using a pipeline of relatively simple morphological operations and filtering (\ref{alg:rbc-detection}). However, this approach led to the detection of false positives \figref{fig:rbc-filter-examples}, with objects such as large platelets and small clusters of \ac{rbc} being detected as \ac{rbc}, and oversegmentation of \ac{rbc} with other objects such as small platelets --- using a small subset, I estimate that approximately 17\% of all objects detected as \ac{rbc} were false positives. As such, I trained an \ac{xgboost} algorithm to automatically filter each object detected based on their morphological description. This performs considerably well, with a 98\% precision, 99\% recall and 98\% specificity on an independent test set, implying that very few correctly detected \ac{rbc} are filtered out ($\approx 1\%$) and only $\approx 2\%$ of all wrongly detected \ac{rbc} are kept for downstream analysis, yielding a post-filtering false positive rate of $\approx 0.34\%$. 

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/rbc-filter-examples.pdf}
    \placecaption{Examples of correctly and incorrectly detected red blood cells (RBC) and their classification by an xgboost model.}
    \label{fig:rbc-filter-examples}
\end{figure}

A median of 20277 were detected in each \ac{wbs} across cohorts (range between 70 and 133916), with more \ac{rbc} being detected on AC1, the cohort used to develop the detection algorithms with no obvious biases dependent on condition or condition subtype being detected except when comparing normal \ac{wbs} with those from individuals with a clinically-relevant condition (\figref{fig:rbc-count-coarse} and \figref{fig:rbc-count-fine}). While controlling for the number of good quality tiles (the ones used in \ac{rbc} detection) and dataset, normal \ac{wbs} are expected to have 11944 \ac{rbc} fewer cells ($p=3*10^{-8}$). 

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/rbc-count-coarse.pdf}
    \placecaption{Detected red blood cells (RBC) stratified by condition (normal, \ac{mds} and anaemia).}
    \label{fig:rbc-count-coarse}
\end{figure}

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/rbc-count-fine.pdf}
    \placecaption{Detected red blood cells (RBC) stratified by relevant condition subtype.}
    \label{fig:rbc-count-fine}
\end{figure}

\subsubsection{White blood cell detection}

The detection of \ac{wbc} was performed with a U-Net model \cite{Ronneberger2015-do} trained on tiles from a few slides from AC1 and validated on tiles from other slides from AC1, MLLC and AC2. I tested different network sizes (multiplying the depth of all layers by a factor of 0.25, 0.5 and 1.0), showing that an intermediate (0.5) size yields the best results (mean \ac{iou} across cohorts = 87\%; \figref{fig:u-net-validation}) which generalize well to the other cohorts being studied \figref{fig:wbc-segmentation-good}. Additionally, it is clear that \ac{tta} improves results consistently (mean \ac{iou} improvement for depth multiplier = 0.5 across all datasets = 1.1\%; \figref{fig:u-net-tta}), making it a simple yet effective addition to improve the results of my \ac{wbc} detection pipeline. This improvement stems mostly from the elimination of relatively small spurious and artefactual regions predicted as belonging to \ac{wbs} as visible in \figref{fig:wbc-segmentation-good}. Early stopping has been reported as an effective strategy to potentially improve the performance of \ac{dl} models \cite{Prechelt2012-xf}. However, in my case, using an earlier epoch did not lead to any improvements in the result for the best performing depth multiplier of 0.5 (\figref{fig:u-net-early-stopping}) which, together with the performance in independent validation sets, hints that training for 100 epochs did not lead to overfitting. However, when considering the case for depth multiplier of 1.0, training for fewer epochs could have led to better results, especially on Adden2; this suggests that overfitting is likely in this model.

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/u-net-validation.pdf}
    \placecaption{U-Net metrics stratified by network size and dataset.}
    \label{fig:u-net-validation}
\end{figure}

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/u-net-tta.pdf}
    \placecaption{Comparison of U-Net performance with and without test-time augmentation (TTA).}
    \label{fig:u-net-tta}
\end{figure}

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/u-net-early-stopping.pdf}
    \placecaption{Comparison of U-Net performance on epoch 50 and epoch 100.}
    \label{fig:u-net-early-stopping}
\end{figure}

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/wbc-segmentation-good.pdf}
    \placecaption{Examples of \ac{wbc} segmentation using a U-Net across three different cohorts. The first column for each represents the input, the second the regular  prediction, the third the prediction using test-time augmentation (TTA) and the fourth shows the effect of refining the prediction after filtering small objects and filling large convex defects. The orange circles represent regions of the image where an improvement between the regular prediction and the prediction using TTA is visible.}
    \label{fig:wbc-segmentation-good}
\end{figure}

I further post-processed each prediction by removing objects detected as \ac{wbc} whose size lies outside the expected distribution for \ac{wbc} sizes, and filling small convex hull defects. I note that this leads to a slight improvement in the prediction (mean \ac{iou} improvement for inference with post-processing across all datasets = 1.2\%; \figref{fig:u-net-post-process}; \figref{fig:wbc-segmentation-good}). 

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/u-net-post-process.pdf}
    \placecaption{Comparison of U-Net performance before and after segmentation post-processing.}
    \label{fig:u-net-post-process}
\end{figure}

Still regarding the performance of this model, it is worth considering a few cases where predictions are of poor quality --- in \figref{fig:wbc-segmentation-bad} I show a three examples of this, noting that these are relatively rare as demonstrated by the high predictive performance of this U-Net model. In essence, these can be classified as one of three types of error --- undersegmentation (\figref{fig:wbc-segmentation-bad}, top), where parts of the \ac{wbc} where not segmented, oversegmentation (\figref{fig:wbc-segmentation-bad}, middle), where the segmented object includes more than just the \ac{wbc}, and false positives ((\figref{fig:wbc-segmentation-bad}, bottom), where the detected object is not a \ac{wbc}. 

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/wbc-segmentation-bad.pdf}
    \placecaption{Examples of poor \ac{wbc} segmentations.}
    \label{fig:wbc-segmentation-bad}
\end{figure}

A median of 936 \ac{wbc} were detected in each \ac{wbs} across cohorts (range between 13 and 36551), with more \ac{wbc} being detected on AC1, the cohort used to develop and train the \ac{wbc} detection algorithms with no obvious biases dependent on condition or condition subtype being detected (\figref{fig:wbc-count-coarse} and \figref{fig:wbc-count-fine}). While controlling for the number of good quality tiles and dataset, no differences were found between conditions. Given that \ac{wbc} counts were available for all individuals in MLLC, I calculate the association between \ac{wbc} counts and the \ac{wbc} density (ratio between the number of detected \ac{wbc} and the number of good quality tiles), showing good association between both ($robust R^2=0.39 [0.30,0.50]$; \figref{fig:wbc-count-association}).

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/wbc-count-coarse.pdf}
    \placecaption{Detected red blood cells (WBC) stratified by condition (normal, \ac{mds} and anaemia).}
    \label{fig:wbc-count-coarse}
\end{figure}

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/wbc-count-fine.pdf}
    \placecaption{Detected red blood cells (WBC) stratified by relevant condition subtype.}
    \label{fig:wbc-count-fine}
\end{figure}

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/wbc-count-association.pdf}
    \placecaption{Association between white blood cell (WBC) density in whole blood slides and WBC counts as measured by complete blood counts ($robust\ R^2=0.39$).}
    \label{fig:wbc-count-association}
\end{figure}

\subsubsection{Characterization of blood cells}

I chose a panel of features that was relatively varied and that captured distinct aspects of cell morphology --- shape, colour and intensity distribution and texture. To provide a more concrete understanding of what a few of these features capture, I provide a few nominal examples of the distribution of these features in both \ac{wbc} and \ac{rbc} in \figref{fig:feature-examples-wbc} and \figref{fig:feature-examples-rbc}. For further clarity, in \figref{fig:feature-examples-wbc} I show examples of variation in terms of cell size (perimeter --- length of the cell edge), shape (circle variance --- how close the shape of the cell approximates that of a circle), nuclear shape (nuclear solidity --- the ratio between the areas of the convex hull of the nucleus and of the nucleus) and granularity (\ac{glcm} (textural) energy --- measures the homogeneity of a region of an image. i.e. the maximum value for the energy is 1 and corresponds to a constant image) for \ac{wbc}. In \figref{fig:feature-examples-rbc}, I show examples of variation in terms of the shape of central pallor (error of the Fourier reconstruction of the colour along the major axis --- measures how complex the colour signal is along the cell) and shape (eccentricty --- ratio between the lengths of the minor and major axis).

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/feature-examples-wbc.pdf}
    \placecaption{Examples of features and how their characterization relates to \ac{wbc} morphology.}
    \label{fig:feature-examples-wbc}
\end{figure}

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/feature-examples-rbc.pdf}
    \placecaption{Examples of features and how their characterization relates to \ac{rbc} morphology.}
    \label{fig:feature-examples-rbc}
\end{figure}

\subsection{Predicting clinical conditions using computational cytomorphology}

\subsubsection{Morphometric distributions discriminate between clinically-relevant conditions}

\noindent \textbf{Independent evaluation of morphometric distributions}

To get a sense on whether the retrieved features could be useful in classification, I first aggregated them on a per-individual basis by calculating the mean and variance of each feature for each cell type (\ac{rbc} and \ac{wbc}). I then used a relatively simple statistical pipeline to test whether there were significant differences between different clinical conditions on MLLC (normal individuals, individuals with iron deficiency anaemia, individuals with megaloblastic anaemia, individuals with \ac{sf3b1} \ac{mds} and individuals with other \ac{mds} subtypes): first, I perform a Kruskal-Wallis test --- an extension of the Mann-Whitney U-test (a sum of ranks based non-parametric test that tests whether a set of samples originate from the same distribution) for multiple samples --- on each individual feature. Then, considering only the features for which the Kruskal-Wallis test was significant after correcting for multiple testing, I performed a post-hoc Dunn test --- a sum of ranks pairwise test for multiple groups that tests if each group pair originates from the same distribution --- to determine how many features are relevant for each comparison (if $p<0.05$ after multiple testing correction).

This analysis shows that, for most cases, a considerable amount of features has sufficient discriminatory power to discriminate between conditions (\figref{fig:dunn-test-heatmap}). I note here that for the remainder of this paragraph I consider a feature to be significant if either its mean or variance is significant. Identifying the presence of a clinically-relevant condition is the easier task, with at least 42 features (79.2\%) and 22 features (52.4\%) offering discriminatory power for \ac{wbc} and \ac{rbc}, respectively. Additionally, discriminating between \ac{mds} and anaemia is a relatively easy task (at least 38 features (71.7\%) and 20 features (47.6\%) for \ac{wbc} and \ac{rbc}, respectively). Finally, it is worth noting that the greatest challenge --- i.e. the comparison for which there are fewer significant features --- lies in the discrimination between \ac{sf3b1}-mutant \ac{mds} and other \ac{mds} subtypes, with 19 features (35.8\%) and 23 features (54.8\%) for \ac{wbc} and \ac{rbc}, respectively. 

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/dunn-test-heatmap.pdf}
    \placecaption{Number of statistically significant features which offer good discriminating power for \ac{wbc} means (top left), \ac{wbc} variance (top right), \ac{rbc} mean (bottom left) and \ac{rbc} variance (bottom right). Colours represent the percentage of features which are statistically-significant.}
\end{figure}

Finally, inspecting the distributions of the two most discriminating features (largest Kruskal-Wallis statistic) reveals already striking differences between features depending on the condition --- regarding the mean of \ac{wbc} features, a decrease on both the ellipse variance (i.e. how distinct a shape is from an ellipse) and the first moment of the \ac{cdf} (a relatively abstract quantifier of shape irregularity) are typical of anaemia or \ac{mds} ($p=2.03*10^{-14}$ and $p=8.44*10^{-16}$ for Mann-Whitney tests comparing the distributions of the means of the ellipse variance and the first moment of the \ac{cdf}, respectively, for normal individuals and individuals with a clinically-relevant condition) and can be used to discriminate between both types of anaemia ($p=3.53*10^{-4}$ and $p=1.72*10^{-4}$ for Mann-Whitney tests comparing distributions of the means of the ellipse variance and the first moment of the \ac{cdf}, respectively, for individuals with either iron deficiency or megaloblastic anaemia; \figref{fig:feature-distribution-mean} (top)), whereas the mean of \ac{rbc} features shows that individuals with \ac{sf3b1}-mutant \ac{mds} have \ac{rbc} with larger perimeters (which can be indicative of large size or shape irregularities) and smaller minimum values for the \ac{cdf} (which can be indicative of smaller sizes or shape irregularities; $p=3.14*10^{-13}$ and $p=9.78*10^{-6}$ for Mann-Whitney tests comparing the distributions of the perimeter and minimum value of the \ac{cdf}, respectively; \figref{fig:feature-distribution-mean} (bottom)).

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/top-features-kw.pdf}
    \placecaption{Examples of the two most discriminating features for \ac{wbc} (top) and \ac{rbc} (bottom) stratified by clinically-relevant condition.}
    \label{fig:feature-distribution-mean}
\end{figure}

\noindent \textbf{Morphometric distributions as features in supervised learning}

These results show the potential of morphometric distributions for diagnostic. However, they also reveal complex relationships between features --- in \ac{rbc}, the perimeter and minimum value of the \ac{cdf} can both be indicative of size but are increased and decreased, respectively, when comparing \ac{sf3b1}-mutant \ac{mds} with other \ac{mds} subtypes. As such, to study these relationships, I test the performance of these features in a 5-fold cross-validated supervised learning setting with two different models --- an elastic network model and a regularized \ac{rf} model --- as described in the methods section. I also use blood counts (\ac{wbcc}, haemoglobin concentration and platelet counts) as covariates in these models, imputing their value as the median whenever these are absent. For clarity, I consider here four distinct binary tasks --- disease detection (normal vs. disease), disease classification (anaemia vs. \ac{mds}), \ac{sf3b1}mut detection (\ac{sf3b1}-mutant \ac{mds} vs. other subtypes of \ac{mds}) and anaemia classification (iron deficiency anaemia vs. megaloblastic anaemia) --- as well as the multi-class scenario (normal vs. \ac{sf3b1}-mutant \ac{mds} vs. other subtypes of \ac{mds} vs. iron deficiency anaemia vs. megaloblastic anaemia). 

Firstly, I assessed which of the models offered the best performance as this can reveal important aspects of how features are associated with one another. Particularly, while a \ac{rf} model is capable of capturing both linear and non-linear relationships, an elastic network model assumes that the relationship between each variable and the label is linear. In \figref{fig:glmnet-vs-rf-auc} and using both the morphometric distributions and blood count data I show that the elastic regression model outperforms the \ac{rf} model for disease detection and classification, while being slightly worst for \ac{sf3b1}mut detection. Additionally, both perform equally well on the task of anaemia classification. As such, I carry on with the elastic regression model, which has the added advantage of allowing the simple calculation of feature group importance.

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/glmnet-vs-rf-auc.pdf}
    \placecaption{Cross-validated \ac{auroc} for the elastic regression model (glmnet; green) and randomized random forest model (RRF; dark red) for the different tasks considered. The average value for each model-task combination is shown as a slanted square.}
    \label{fig:glmnet-vs-rf-auc}
\end{figure}

I then trained models using only blood counts, only morphology (morphometric profiles), or both. I show that morphology often outperforms blood counts across all tasks, with a considerable improvement when considering disease classification (\figref{fig:roc-curves-binary} and \figref{fig:auc-binary}), where using only blood counts is considerably worst. Importantly, I note that in all cases the performance is best when using both types of data. Interestingly, using blood counts alone is relatively effective at detecting the presence of a \ac{sf3b1} mutation in an individual with \ac{mds} (AUC = \missing{AUC}).

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/roc-curves-binary.pdf}
    \placecaption{Cross-validated receiver operating characteristic curves for the four tasks considered. Coloured depending on the data type used --- blood counts (B.C.), morphology or both (morphology + B.C.).}
    \label{fig:roc-curves-binary}
\end{figure}

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/auc-binary.pdf}
    \placecaption{\Ac{auroc} values for the four tasks considered. Coloured depending on the data type used --- blood counts (B.C.), morphology or both (morphology + B.C.).}
    \label{fig:auc-binary}
\end{figure}

Inspecting how different features impact prediction using both blood counts and morphology can reveal some important trends. For instance and for disease detection, lower haemoglobin levels, an excess of average \ac{rbc} eccentricity (the ratio between major and minor axes) and average \ac{wbc} nuclear green mass displacement (measures how much the red colour distribution is shifted from the centre, thus quantifying nuclear irregularities) contributes considerably to the "normal" classification (\figref{fig:feature-importance-disease-detection-classification} (left)). Within individuals with a disease (anaemia or \ac{mds}), it becomes clear that low platelet and \ac{wbc} counts, together with high haemoglobin concentration are hallmarks of \ac{mds}. An increase in the average \ac{rbc} perimeter, decrease in the average \ac{wbc} nuclear eccentricity, an increase in the variance of \ac{wbc} solidity (the ratio between the object and its convex hull, quantifying deformations to the shape) can also be important in diagnostic of \ac{mds} (\figref{fig:feature-importance-disease-detection-classification} (right)).

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/feature-importance-disease-detection-classification.pdf}
    \placecaption{Coefficients for the five most important features in the elastic network regression (glmnet) model by feature group (blood counts (B.C.), \ac{rbc} mean, \ac{rbc} variance, \ac{wbc} mean and \ac{wbc} variance) for disease detection (left) and disease classification (right).}
    \label{fig:feature-importance-disease-detection-classification}
\end{figure}

As for the discrimination between disease subtypes shows that, for the most part, high platelet counts are indicative of \ac{sf3b1}-mutant \ac{mds} (\figref{fig:feature-importance-mds}) as observed earlier in \figref{fig:mds-plt}. Additionally, an increase in the average \ac{rbc} perimeter is also predictive of \ac{sf3b1}-mutant \ac{mds}. Regarding the classification of different anaemia subtypes, an increase in platelet counts is expected in iron deficiency anaemia, whereas an increase in the \ac{rbc} average minimum of the minor axis peak profile (a measure of the colour distribution along the major axis of the cell) is expected in megaloblastic anaemia \figref{fig:feature-importance-anaemia}. This can point towards a greater prevalence of spherocytes in iron deficiency anaemia. Additionally, a seemingly unintuitive finding which will be further confirmed ahead is that morphological aspects of \ac{wbc} --- particularly concerning the nuclei of \ac{wbc} --- contribute heavily towards the classification of different subtypes of anaemia, a disease affecting \ac{rbc}. However, this can be explained by the fact that hypersegmented neutrophils --- neutrophils with more than 5 nuclear segments --- are overrepresented in \ac{wbs} in megaloblastic anaemia \cite{Hariz2021-qw}.

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/feature-importance-mds.pdf}
    \placecaption{Coefficients for the important features in the elastic network regression (glmnet; abs(coefficient) > 0) model by feature group (blood counts (B.C.), \ac{rbc} mean, \ac{rbc} variance, \ac{wbc} mean and \ac{wbc} variance) for \ac{sf3b1} mutation detection.}
    \label{fig:feature-importance-mds}
\end{figure}

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/feature-importance-anaemia.pdf}
    \placecaption{Coefficients for the important features in the elastic network regression (glmnet; abs(coefficient) > 0) model by feature group (blood counts (B.C.), \ac{rbc} mean, \ac{rbc} variance, \ac{wbc} mean and \ac{wbc} variance) for \ac{sf3b1} mutation detection.}
    \label{fig:feature-importance-anaemia}
\end{figure}

Finally, looking at how groups of features impact prediction shows that morphometric distributions account for most of the variance explained by these predictive models, with aspects pertaining to both mean and variance playing a significant role (\figref{fig:feature-group-importance}). An obvious exception is the detection of \ac{sf3b1} mutations in individuals with \ac{mds} --- here, it is blood counts, particularly platelet counts, that drive the classification, with \ac{wbc} playing a practically negligible part. This may be due to the fact that \ac{wbc} abnormalities are present only in a few cells in \ac{mds}, making the use of relatively generic distribution descriptors (i.e. mean and variance) not adequate for classification.

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/feature-group-importance.pdf}
    \placecaption{Feature group importance for the prediction of different conditions by feature group (blood counts (B.C.), \ac{rbc} mean, \ac{rbc} variance, \ac{wbc} mean and \ac{wbc} variance). Each circle is scaled according to the explained variance.}
    \label{fig:feature-group-importance}
\end{figure}

Finally, reconsidering this problem as a multiple class problem and training models for each data type further highlights the value of morphology in classification --- the multiple class \ac{auroc} is considerably higher morphology (91.3\%) than for blood counts only (83.6\%; \figref{fig:multiclass-auc}), an effect driven largely by the poor predictive ability of blood counts in discriminating between \ac{sf3b1}-mutant \ac{mds} (AUC = 33\%) and between megaloblastic anaemia and non-\ac{sf3b1}-mutant anaemia (AUC = 72\%; \figref{fig:multiclass-auc-heatmap}). As is the case for the binary classification tasks, using information on both blood counts and morphology is the best option (AUC = 93.6\%).

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/multiclass-auc.pdf}
    \placecaption{Multiple class \ac{auroc}.}
    \label{fig:multiclass-auc}
\end{figure}

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/multiclass-auc-heatmap.pdf}
    \placecaption{Multiple class pairwise \ac{auroc} for different tasks and data types.}
    \label{fig:multiclass-auc-heatmap}
\end{figure}

This section covers the potential of morphometric profiles in haematological diagnostic. However, it also highlights an important caveat of this method --- while it is relatively simple to understand what an increase on the average perimeter of \ac{rbc} (as observable in cases of \ac{sf3b1}-mutant \ac{mds} when compared with other \ac{mds} subtypes) or even a decrease in the variability of solidity in \ac{wbc} (as observable in cases of \ac{mds} when compared with anaemias), some of these features do not translate immediately to evident visual features, especially when the variation of these features in a single individual is the relevant aspect of classification. Additionally, as is the case for \ac{sf3b1}mut detection, alterations may be present in a small subset of cells, rendering morphometric distributions less suitable to capturing relevant morphological signatures. For this reason, I developed \ac{mile-vice}, a method capable of creating visually coherent groups of cells which are relevant for disease classification with minimal human input.

\subsubsection{MILe-ViCe identifies visually coherent and clinically relevant groups of cells}

Multiple Instance Learning for Virtual Cell --- \ac{mile-vice} --- is a \ac{mil} method capable of clustering collections (regularly known as bags in the \ac{mil} field) of cells in a way that makes their prevalence relevant for classification. I trained models to perform the same tasks as those in the previous section described in the methods' section of this Chapter ---  disease detection, disease classification, \ac{sf3b1}mut \ac{mds} detection and anaemia classification, as well as a multi-objective scenario where the same set of cells is used to optimize different binary tasks. For clarity, I will also call the binary class tasks as "single objective" tasks. To reduce the search space for \ac{mile-vice}, I select the subset of features whose mean or variance had a coefficient with an absolute value greater than 0.01 in the elastic network regression model.

\noindent \textbf{Algorithm performance}

\ac{mile-vice} has to assume that cells can be clustered into a given number of virtual cells. I tested three different values for the number of virtual cells in the binary classification tasks (10, 25 and 50) and two different values for the multi-objective task (25 and 50). I also test whether including blood count data can help improve results \figref{fig:mile-vice-performance}. The number of virtual cells has an impact on the median cross-validation \ac{auroc}:

\begin{itemize}
    \item In the \textbf{single objective} setting using \textbf{morphology}, the best performing models are generally the ones with 25 virtual cells excluding the case of anaemia classification, for which the best performing model considers 10 virtual cells;
    \item In the \textbf{single objective} setting using \textbf{morphology and blood counts}, the best performing models have 50 virtual cells excluding the case of anaemia classification, for which the best performance is observed for the model considering 10 virtual cells;
    \item In the \textbf{multiple objective} setting using \textbf{morphology}, the best performing models have 50 virtual cells excluding the case of anaemia classification. In this case, the best performing model considers 25 virtual cells;
    \item \item In the \textbf{multiple objective} setting using \textbf{morphology and blood counts}, the best performing models have 50 virtual cells excluding the case of disease classification, for which the best performing model has 25 virtual cells.
\end{itemize}

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/mile-vice-cv-performance.pdf}
    \placecaption{\Ac{auroc} for \ac{mile-vice} models with different numbers of virtual cells and using different types of data --- "Morphology" (only using image features retrieved from cells in the slide) and "Morphology + B.C." (using image features from cells and blood counts). The models with the best performance for each set of virtual cell numbers are highlighted with black rectangles.}
    \label{fig:mile-vice-performance}
\end{figure}

While performance does not vary widely between different numbers of virtual cells, this analysis highlights that for some tasks a smaller number of virtual cells is beneficial --- this may be associated with the over-parametrization of models with higher numbers of virtual cells in specific tasks. Additionally, I also note that the multi-objective scenario leads to slightly worse performance (\figref{fig:mile-vice-performance} and \figref{fig:mile-vice-roc-curves}). However, the multi-objective has a clear advantage --- while the single-objective performance can be slightly better, it also offers a less compact representation, with a higher number of possible virtual cells.

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/mile-vice-roc-curve.pdf}
    \placecaption{\Ac{auroc} for \ac{mile-vice} models with different numbers of virtual cells and using different types of data --- "Morphology" (only using image features retrieved from cells in the slide) and "Morphology + B.C." (using image features from cells and blood counts). The models with the best performance for each set of virtual cell numbers are highlighted with black rectangles.}
    \label{fig:mile-vice-performance}
\end{figure}

Comparing \ac{mile-vice} with the elastic linear regression models trained on morphometric distributions shows that \ac{mile-vice} offers better performance (\figref{fig:mile-vice-vs-glmnet} and \figref{fig:mile-vice-vs-glmnet-scatter}) in both the single and multiple objective settings. The only exception is for the task of disease classification, where using \ac{mile-vice} with morphology and blood counts leads to slightly worse performance in both single and multiple objective settings, and using using \ac{mile-vice} with morphology leads to slightly worse performance in the multiple objective setting. Overall, however, I show here that \ac{mile-vice} offers comparable performance while avoiding aggregating morphometric data in a class-agnostic way. 

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/mile-vice-roc-curve.pdf}
    \placecaption{Barplot comparing the cross-validated \ac{auroc} between \ac{mile-vice} and the elastic linear regression model trained on morphometric features (glmnet).}
    \label{fig:mile-vice-vs-glmnet}
\end{figure}

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/mile-vice-roc-curve-scatter.pdf}
    \placecaption{Scatterplot comparing the cross-validated \ac{auroc} between \ac{mile-vice} and the elastic linear regression model trained on morphometric features (glmnet).}
    \label{fig:mile-vice-vs-glmnet-scatter}
\end{figure}

\noindent \textbf{Virtual cell inspection and validation}

After showing that \ac{mile-vice} is capable of discriminating different clinically relevant conditions, performing almost always better than the elastic network regression model using the distribution parameters of differnt morphometric features, I inspect specific virtual cell groups to better understand which ones are contributing the most towards classification. To this effect, I retrieve the absolute coefficient values for the classification layer from the best performing \ac{mile-vice} model for each task and multiply it by the cell proportions in the MLLC cohort, thus obtaining an effect-size for each virtual cell type. I then use this effect size to rank the most and least important virtual cell types, taking the 5 most important cells for classification (the 5 cells with the highest absolute effect size). The final step is the careful observation of the more important virtual cell types (the virtual cell types with the highest effect size) in order to determine how these cell types relate to cytomorphological phenotypes and define cellular (\ac{rbc} and \ac{wbc}) archetypes. I refer here to "morphology-only model" as the models which consider only morphological aspects of cells, and "complete model" as the model which, apart from morphology, also considers \ac{cbc}.

\paragraph{Disease detection.} In disease detection (normal vs. disease), it is evident that \ac{mile-vice} does a good job at descriminating between cell types which are more prevalent in each condition when using only morphology and when using both morphology and \ac{cbc} (B.C.; \figref{fig:mile-vice-vcq-so-disease-detection}). More particularly, when considering \ac{rbc}, relatively large \ac{rbc} (high perimeter) are one of the \ac{rbc} archetypes detected by our model as hallmarks of the disease classification (25, 42 and 5 in the morphology-only model and 44, 16, 35 and 19 in the complete model; \figref{fig:rbc-disease-detection-examples-disease}). Additionally, rounder \ac{rbc} (low eccentricity) are \ac{rbc} archetypes of the normal classification (45 in the morphology-only model and 37 in the complete model \figref{fig:rbc-disease-detection-examples-normal}). However, it should be noted that \ac{rbc} homogeneity makes it complicated to assign specific and well-defined \ac{rbc} archetypes. These \ac{rbc} archetypes make sense --- a hallmark of \ac{mds} and megaloblastic anaemia is indeed enlarged \ac{rbc}, while the presence of non-circular \ac{rbc} (poikilocytosis) is a general hallmark of aberrant \ac{rbc} production as observed in both \ac{mds} and anaemia \cite{Ford2013-nd}. 

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/mile-vice-vcq-so-disease-detection.pdf}
    \placecaption{The effect size and average proportion ratio for the five most relevant virtual cell types for both red blood cells (RBC) and white blood cells (WBC) for disease detection (disease vs. normal) when using only morphological features (Morphology) and when using both morphological features and complete blood counts (Morphology + B.C.). Positive effect values/higher proportion ratio are associated with classification as normal, whereas negative effect values/lower proportion ratio are associated with classification as disease.}
    \label{fig:mile-vice-vcq-so-disease-detection}
\end{figure}

A few \ac{wbc} archetypes can also be pointed out (\figref{fig:mile-vice-vcq-so-disease-detection}) --- virtual cell type 20 in the morphology-only model and 4 in the complete model contain neutrophils featuring nucelar abnormalities (Pelger-Huët anomalies, characterized by either U- or dumbbell-shaped nuclei \cite{Bain2005-uq} and nuclear oversegmentation), whereas virtual cell type features, among other types of cells, a relatively high proportion of blasts \figref{fig:wbc-disease-detection-examples-disease}. It is worth highlight that, specifically for this task, the identified \ac{wbc} archetypes are somewhat varied, probably due to the considerable variability of \ac{wbc} in anaemias and, more particularly, in \ac{mds} \cite{Bain2005-uq,Bain2014-oc}.

It is worth noting at this point that \ac{rbc} are relatively homogeneous and trying to identify specific subgroups is quite challenging across all predictive tasks. Additionally, the analysis in the previous subsection illustrates how some changes in easily interpreatable \ac{rbc} feature distributions --- such as increases in their average perimeter --- can be informative for prediction. For this reason, I have focused mainly on \ac{wbc} archetypes. Additionally, I will focus on the virtual cell types important for the classification while using \ac{cbc} --- this allows me to focus aspects of condition prediction which are not captured by \ac{cbc} as these are relatively simple to obtain. 

\paragraph{Disease classification.} In disease classification (anaemia vs. \ac{mds}), an \ac{rbc} archetype typical of anaemia is the presence of dacrocytes (tear-shaped \ac{rbc}) and eliptocytes (elongated \ac{rbc}; 25 in the complete model), whereas larger \ac{rbc} are expected in \ac{mds} (15 and 23 in the complete model; \figref{fig:mile-vice-vcq-so-disease-classification} and \figref{fig:rbc-disease-classification-examples}). Regarding \ac{wbc} archetypes, the presence of normal lymphocytes and hyperlobulated neutrophils are hallmarks of anaemia (18 and 14 in the complete model, respectively), whereas Pelger-Huët anomalies/band cells and immature myeloid lineage \ac{wbc} are more typical of \ac{mds} (9 and 24 in complete model, respectively; \figref{fig:wbc-disease-classification-examples}). Here, it is important to note that the task of distinguishing between band cells (immature and hypolobulated) and Perger-Huët anomalies is non-trivial --- while both can be characterized as granulocytes with U-shaped nuclei, it is worth noting that band cells have slightly larger nuclei and less densely packed chromatin (i.e. the staining appears lighter under the microscope) \cite{Colella2012-so}. These fine-grained differences are likely to go undifferentiated within \ac{mile-vice} in the task of discriminating \ac{mds} and anaemia, particularly since both cell types have been observed in cases of \ac{mds} \cite{Davey1988-zn,Cunningham1995-pc}.

\begin{figure}[!ht]
    \placefigure{gfx/cytomorphology/mile-vice-vcq-so-disease-classification.pdf}
    \placecaption{The effect size and average proportion ratio for the five most relevant virtual cell types for both red blood cells (RBC) and white blood cells (WBC) for disease classification (anaemia vs. \ac{mds}) when using only morphological features (Morphology) and when using both morphological features and complete blood counts (Morphology + B.C.). Positive effect values/higher proportion ratio are associated with classification as \ac{mds}, whereas negative effect values/lower proportion ratio are associated with classification as anaemia.}
    \label{fig:mile-vice-vcq-so-disease-classification}
\end{figure}

\paragraph{\ac{sf3b1}-mutation detection.} 

\subsection{External validation}

\subsubsection{Evidence for covariate shift}

\section{Discussion}

In this chapter I outline a methodology capable of detecting and characterizing between hundreds and hundreds of thousands of cells (\ac{rbc} and \ac{wbc}) in \ac{wbs} and apply it to three distinct cohorts. Using one of these cohorts --- MLLC --- I show how the protocol here outlined can be used to create individual and clinically-relevant cytomorphological profiles based on morphometric distributions, applying them successfully to the prediction of different haematological conditions. Additionally, using vocabulary-based \ac{mil}, a \ac{ml} paradigm focused on the discovery of latent terms which are relevant for the classification of bags of instances, can be used to discover virtual cells types, which can be used as cytomorphological hallmarks of different haematological conditions while performing adequately in most predictive tasks.

\paragraph{Computational cytomorphology detects thousands of blood cells.} I show that computational cytomorphology is a valuable tool to detect thousands of blood cells in \ac{wbs}. This creates large collections of cells which can be used by experts not only for diagnostic and prognostic purposes, but also for pedagogy and facilitated cell type discovery --- by creating widely accessible collections of cells, the burden of carefully analysing cells along a \ac{wbs} becomes facilitated. The development of \ac{tbca}, an online tool for blood cell annotation, is also useful for this --- by aggregating blood cells, annotation and its verification become a relatively simple task for experts. Additionally, a centralized platform also enables annotations to be crowdsourced, making them more robust and less prone to subjective interpretation. 

\paragraph{Computational cytomorphology enables the prediction of clinically-relevant conditions.} enabling the prediction of distinct conditions using only \ac{wbs}. I uncover specific cellular subtypes associated with each condition, showing how weak supervision in a \ac{mil} setting can be used to derive powerful conclusions regarding the specific cytomorphological signatures which characterize each condition. These cellular subtypes were further confirmed to capture specific cell types and their relative prevalence across different conditions was used to verify some previously stated conclusions --- particularly that megaloblastic anaemia has an over-representation of hyper-segmented neutrophils --- and to uncover novel associations --- band cells (i.e. immature neutrophils) are more common in \ac{sf3b1}-mutant \ac{mds} than in other \ac{mds} subtypes. % etc. etc.

\paragraph{Towards multi-centre blood cytomorphology.} External validation shows this methodology has some generalization abilities which are not sufficient for its application in other centres --- particularly for \ac{mile-vice}, the detection of \ac{sf3b1}-mutant \ac{mds} suffers a small drop in \ac{auroc}, whereas the detection of diseases (\ac{mds} or anaemia) performs poorly when tested on an independent test set, an effect that likely stems from covariate shift --- a change in the probability distribution of the data. More particularly, this can be formalized as the probability distribution of the training data ($\mathrm{P}(X_{MLLC})$) being distinct from that of the external validation data as shown above ($\mathrm{P}(X_{AC2})$). This highlights an important aspect of works in digital histopathology which has been stated in recent reviews --- multi-centre training sets, allowing \ac{ml} algorithms to learn how to control for dataset origin (which can affect, among other aspects, image compression, illumination, and stain intensity and distribution), are an increasing necessity if good generalization is to be expected \cite{Van_der_Laak2021-id}, especially in settings where images are obtained mainly for clinical, rather than scientific, use.

\section{Code availability}

The code for different sections of this work is available across different repositories:

\begin{itemize}
    \item Code to train, test and predict using the quality control network is available in \url{}
    \item Code to train, test and predict using the U-Net model is available in \url{}
    \item Code to train and test the automatic \ac{rbc} object filter is available in \url{}
    \item The cell detection and characterization pipeline is available in \url{}
    \item R scripts and code to train the elastic network regression model and the regularized random forest model, and generate figures is available in \url{}
    \item The code to train, test and predict using \ac{mile-vice} is available in \url{}
\end{itemize}
